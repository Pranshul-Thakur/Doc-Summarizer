{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6169cc47-a314-4feb-a336-d9137ccae634",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "from typing import List\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.agents import Tool, initialize_agent, AgentType\n",
    "from langchain.schema import Document\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from crewai import Agent, Task, Crew\n",
    "import litellm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "eec8b4b7-e3b1-4bd9-9dca-e5924718ec95",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(\n",
    "    role=\"Reader\",\n",
    "    goal=\"Extract relevant info\",\n",
    "    backstory=\"Research assistant\",\n",
    "    llm= ChatGoogleGenerativeAI (\n",
    "    model=\"gemini-1.5-flash-latest\",\n",
    "    temperature=0,\n",
    "    google_api_key=\"AIzaSyD42WOFIkDCf3uVXmTOsaBWjVPdOJWRT5E\"\n",
    "    ) \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e919aefa-eb9c-4f4a-b3d3-48a436f7bddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDINGS = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "124c4d72-39b4-499f-b928-700a9c009713",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHROMA_DIR = \"./vector_store\"\n",
    "RAG_COLLECTION = \"research_rag\"\n",
    "MEM_COLLECTION = \"long_term_memory\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5bb1bd3e-d04e-461c-ad11-c7187208e68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_docs(raw_docs: List[Document]) -> List[Document]:\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)\n",
    "    return splitter.split_documents(raw_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "25b3ca44-8a5a-4707-a9d1-998d8924ee39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PDF_to_docs(fp: str) -> List[Document]:\n",
    "    return split_docs(PyPDFLoader(fp).load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "256ae0b9-7ff8-4a1d-9121-115f12b3fc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DOCX_to_docs(fp: str) -> List[Document]:\n",
    "    return split_docs(Docx2txtLoader(fp).load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "060a7112-7f8a-4e10-a254-f699e8a4776b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_docs(path: str) -> List[Document]:\n",
    "    docs = []\n",
    "    if os.path.isfile(path):\n",
    "        if path.lower().endswith(\".pdf\"):\n",
    "            docs.extend(PDF_to_docs(path))\n",
    "        elif path.lower().endswith((\".docx\", \".doc\")):\n",
    "            docs.extend(DOCX_to_docs(path))\n",
    "    else:\n",
    "        for root, _, files in os.walk(path):\n",
    "            for f in files:\n",
    "                fp = os.path.join(root, f)\n",
    "                if f.lower().endswith(\".pdf\"):\n",
    "                    docs.extend(PDF_to_docs(fp))\n",
    "                elif f.lower().endswith((\".docx\", \".doc\")):\n",
    "                    docs.extend(DOCX_to_docs(fp))\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "01665304-17c9-4e08-aff4-1943421bc52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_or_load_vs(collection: str) -> Chroma:\n",
    "    return Chroma(collection_name=collection, embedding_function=EMBEDDINGS, persist_directory=CHROMA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b2abd043-4296-4650-b851-179ca40c7d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def memory_write(text: str):\n",
    "    vs = build_or_load_vs(MEM_COLLECTION)\n",
    "    vs.add_texts([text])\n",
    "    vs.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d03cc24f-8a21-4146-b6ac-739afe7c6d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def memory_search(query: str, k: int = 5) -> List[str]:\n",
    "    vs = build_or_load_vs(MEM_COLLECTION)\n",
    "    return [d.page_content for d in vs.similarity_search(query, k=k)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f5fa13b5-9271-4687-bee2-8bfb0a7098fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_retriever():\n",
    "    return build_or_load_vs(RAG_COLLECTION).as_retriever(search_kwargs={\"k\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "014f6253-b8ac-4386-b46c-4c17d9793e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tool_retrieve(query: str) -> str:\n",
    "    chain = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=rag_retriever(), return_source_documents=True)\n",
    "    out = chain({\"query\": query})\n",
    "    ans = out[\"result\"]\n",
    "    sources = \"\\n\".join({d.metadata.get(\"source\", \"unknown\") for d in out[\"source_documents\"]})\n",
    "    return f\"Answer: {ans}\\n\\nSources:\\n{sources}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "81a45c24-c379-49c7-97da-b619d5b4752c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tool_summarize(path: str) -> str:\n",
    "    docs = PDF_to_docs(path) if path.lower().endswith(\".pdf\") else DOCX_to_docs(path)\n",
    "    chunks = [d.page_content for d in docs]\n",
    "    summaries = []\n",
    "    for i in range(0, len(chunks), 6):\n",
    "        part = \"\\n\\n\".join(chunks[i:i+6])\n",
    "        resp = llm.invoke([(\"system\", \"Summarize in bullet points <=120 words\"), (\"user\", part[:6000])])\n",
    "        summaries.append(resp.content)\n",
    "    final_resp = llm.invoke([(\"system\", \"Combine into one <=200 words\"), (\"user\", \"\\n\\n\".join(summaries)[:6000])])\n",
    "    return final_resp.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fbd0356b-fe2b-4fe0-a185-de432f50984d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tool_memory_write(text: str) -> str:\n",
    "    memory_write(text)\n",
    "    return \"Saved to long-term memory.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "60a76ce7-2630-4c1e-8230-a778ef005489",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tool_memory_search(query: str) -> str:\n",
    "    hits = memory_search(query)\n",
    "    return \"\\n- \".join([\"Memories:\"] + hits) if hits else \"No memory found.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "53ed7723-69f9-4cc5-900a-df5c4417b48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_agent():\n",
    "    tools = [\n",
    "        Tool(name=\"retrieve\", func=tool_retrieve, description=\"Answer research questions from indexed docs\"),\n",
    "        Tool(name=\"summarize_file\", func=tool_summarize, description=\"Summarize a PDF/DOC file\"),\n",
    "        Tool(name=\"memory_write\", func=tool_memory_write, description=\"Store a note in long-term memory\"),\n",
    "        Tool(name=\"memory_search\", func=tool_memory_search, description=\"Search notes in long-term memory\"),\n",
    "    ]\n",
    "    return initialize_agent(tools=tools, llm=llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c321ab86-fb25-4741-bd48-bd64a513da8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest(path: str):\n",
    "    docs = load_docs(path)\n",
    "    vs = build_or_load_vs(RAG_COLLECTION)\n",
    "    vs.add_texts([d.page_content for d in docs], metadatas=[d.metadata for d in docs])\n",
    "    vs.persist()\n",
    "    print(f\"Ingested {len(docs)} chunks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c3de5950-beb8-4792-b41a-5d8ca00f2cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_cli():\n",
    "    agent = make_agent()\n",
    "    print(\"Type 'exit' to quit.\")\n",
    "    while True:\n",
    "        q = input(\"You: \")\n",
    "        if q.strip().lower() in {\"exit\", \"quit\"}:\n",
    "            break\n",
    "        try:\n",
    "            print(agent.run(q))\n",
    "        except Exception as e:\n",
    "            print(f\"[Error] {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0e8b9a35-f2ff-492f-a8aa-8050978c04b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_cli(path: str):\n",
    "    print(tool_summarize(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "71b49f52-fc4f-4ac8-820d-ab1a7037f998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingested 36 chunks.\n"
     ]
    }
   ],
   "source": [
    "ingest(\"C:\\\\Users\\\\LENOVO\\\\Desktop\\\\new\\\\main\\\\App\\\\Research_paper.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8c3a4f34-b74f-4bb5-9b26-379558bf5fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.chains import RetrievalQA\n",
    "\n",
    "# def summarize_docs() -> str:\n",
    "#     vs = build_or_load_vs(RAG_COLLECTION)  # load your ingested vector store\n",
    "#     chain = RetrievalQA.from_chain_type(\n",
    "#         llm=llm,\n",
    "#         retriever=vs.as_retriever(search_kwargs={\"k\": 5}),\n",
    "#         chain_type=\"stuff\"\n",
    "#     )\n",
    "#     return chain.run(\"Summarize the ingested document in detail.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0aef8c77-ba5b-4ded-9c51-dc030bfe903c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary = summarize_docs()\n",
    "# print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ea270b41-58b2-42c9-b00c-7afe2c4b1cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "vs = build_or_load_vs(RAG_COLLECTION)\n",
    "retriever = vs.as_retriever(search_kwargs={\"k\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a15bb4f2-d88c-4789-972b-57d9cdbc4197",
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = Agent(\n",
    "    role=\"Reader\",\n",
    "    goal=\"Extract relevant information from retrieved chunks of the document\",\n",
    "    backstory=\"A careful researcher that can understand academic text and pick key details.\",\n",
    "    llm=llm\n",
    ")\n",
    "\n",
    "summarizer = Agent(\n",
    "    role=\"Summarizer\",\n",
    "    goal=\"Summarize the extracted information into a concise but detailed summary\",\n",
    "    backstory=\"An academic writer skilled at condensing complex information.\",\n",
    "    llm=llm\n",
    ")\n",
    "\n",
    "reviewer = Agent(\n",
    "    role=\"Reviewer\",\n",
    "    goal=\"Verify clarity and accuracy of the summary, improving readability\",\n",
    "    backstory=\"A proofreader ensuring correctness and style.\",\n",
    "    llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "de0a5c5e-db18-4ed2-bc58-84f16aa54641",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = Task(\n",
    "    description=\"Use the retriever object (already available in memory) to fetch the most relevant chunks for query: {query}.\",\n",
    "    agent=reader,\n",
    "    expected_output=\"A set of key passages or bullet points extracted from the retriever.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b71365c7-3526-4340-b081-abf2d2891a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 = Task(\n",
    "    description=\"Summarize the extracted content into 5-7 bullet points that cover the document's main ideas.\",\n",
    "    agent=summarizer,\n",
    "    expected_output=\"A concise summary in bullet points.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "422d35a2-8120-4742-8e26-214fa8aba7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "t3 = Task(\n",
    "    description=\"Review the summary for clarity, accuracy, and readability. Fix grammar and improve flow.\",\n",
    "    agent=reviewer,\n",
    "    expected_output=\"A polished final summary ready to present.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "23b49d38-5148-4f92-b0b8-c51bfc743e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "crew = Crew(agents=[reader, summarizer, reviewer], tasks=[t1, t2, t3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7c9a98a7-ba2b-477f-b2cf-8fcbfd0f38b6",
   "metadata": {},
   "outputs": [
    {
     "ename": "BadRequestError",
     "evalue": "litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=models/gemini-1.5-flash-latest\n Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mBadRequestError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[65]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m result = \u001b[43mcrew\u001b[49m\u001b[43m.\u001b[49m\u001b[43mkickoff\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mquery\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mSummarize the ingested document in detail\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\desktop\\new\\main\\Lib\\site-packages\\crewai\\crew.py:661\u001b[39m, in \u001b[36mCrew.kickoff\u001b[39m\u001b[34m(self, inputs)\u001b[39m\n\u001b[32m    658\u001b[39m     \u001b[38;5;28mself\u001b[39m._handle_crew_planning()\n\u001b[32m    660\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.process == Process.sequential:\n\u001b[32m--> \u001b[39m\u001b[32m661\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_sequential_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    662\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.process == Process.hierarchical:\n\u001b[32m    663\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._run_hierarchical_process()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\desktop\\new\\main\\Lib\\site-packages\\crewai\\crew.py:772\u001b[39m, in \u001b[36mCrew._run_sequential_process\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    770\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_run_sequential_process\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> CrewOutput:\n\u001b[32m    771\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Executes tasks sequentially and returns the final output.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m772\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_execute_tasks\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\desktop\\new\\main\\Lib\\site-packages\\crewai\\crew.py:875\u001b[39m, in \u001b[36mCrew._execute_tasks\u001b[39m\u001b[34m(self, tasks, start_index, was_replayed)\u001b[39m\n\u001b[32m    872\u001b[39m     futures.clear()\n\u001b[32m    874\u001b[39m context = \u001b[38;5;28mself\u001b[39m._get_context(task, task_outputs)\n\u001b[32m--> \u001b[39m\u001b[32m875\u001b[39m task_output = \u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute_sync\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m    \u001b[49m\u001b[43magent\u001b[49m\u001b[43m=\u001b[49m\u001b[43magent_to_use\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mList\u001b[49m\u001b[43m[\u001b[49m\u001b[43mBaseTool\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools_for_task\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m task_outputs.append(task_output)\n\u001b[32m    881\u001b[39m \u001b[38;5;28mself\u001b[39m._process_task_result(task, task_output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\desktop\\new\\main\\Lib\\site-packages\\crewai\\task.py:364\u001b[39m, in \u001b[36mTask.execute_sync\u001b[39m\u001b[34m(self, agent, context, tools)\u001b[39m\n\u001b[32m    357\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mexecute_sync\u001b[39m(\n\u001b[32m    358\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    359\u001b[39m     agent: Optional[BaseAgent] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    360\u001b[39m     context: Optional[\u001b[38;5;28mstr\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    361\u001b[39m     tools: Optional[List[BaseTool]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    362\u001b[39m ) -> TaskOutput:\n\u001b[32m    363\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Execute the task synchronously.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m364\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_execute_core\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\desktop\\new\\main\\Lib\\site-packages\\crewai\\task.py:512\u001b[39m, in \u001b[36mTask._execute_core\u001b[39m\u001b[34m(self, agent, context, tools)\u001b[39m\n\u001b[32m    510\u001b[39m \u001b[38;5;28mself\u001b[39m.end_time = datetime.datetime.now()\n\u001b[32m    511\u001b[39m crewai_event_bus.emit(\u001b[38;5;28mself\u001b[39m, TaskFailedEvent(error=\u001b[38;5;28mstr\u001b[39m(e), task=\u001b[38;5;28mself\u001b[39m))\n\u001b[32m--> \u001b[39m\u001b[32m512\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\desktop\\new\\main\\Lib\\site-packages\\crewai\\task.py:428\u001b[39m, in \u001b[36mTask._execute_core\u001b[39m\u001b[34m(self, agent, context, tools)\u001b[39m\n\u001b[32m    426\u001b[39m \u001b[38;5;28mself\u001b[39m.processed_by_agents.add(agent.role)\n\u001b[32m    427\u001b[39m crewai_event_bus.emit(\u001b[38;5;28mself\u001b[39m, TaskStartedEvent(context=context, task=\u001b[38;5;28mself\u001b[39m))\n\u001b[32m--> \u001b[39m\u001b[32m428\u001b[39m result = \u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute_task\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    429\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    430\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    431\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    432\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    434\u001b[39m pydantic_output, json_output = \u001b[38;5;28mself\u001b[39m._export_output(result)\n\u001b[32m    435\u001b[39m task_output = TaskOutput(\n\u001b[32m    436\u001b[39m     name=\u001b[38;5;28mself\u001b[39m.name,\n\u001b[32m    437\u001b[39m     description=\u001b[38;5;28mself\u001b[39m.description,\n\u001b[32m   (...)\u001b[39m\u001b[32m    443\u001b[39m     output_format=\u001b[38;5;28mself\u001b[39m._get_output_format(),\n\u001b[32m    444\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\desktop\\new\\main\\Lib\\site-packages\\crewai\\agent.py:458\u001b[39m, in \u001b[36mAgent.execute_task\u001b[39m\u001b[34m(self, task, context, tools)\u001b[39m\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m e.\u001b[34m__class__\u001b[39m.\u001b[34m__module__\u001b[39m.startswith(\u001b[33m\"\u001b[39m\u001b[33mlitellm\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    449\u001b[39m     \u001b[38;5;66;03m# Do not retry on litellm errors\u001b[39;00m\n\u001b[32m    450\u001b[39m     crewai_event_bus.emit(\n\u001b[32m    451\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    452\u001b[39m         event=AgentExecutionErrorEvent(\n\u001b[32m   (...)\u001b[39m\u001b[32m    456\u001b[39m         ),\n\u001b[32m    457\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m458\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    459\u001b[39m \u001b[38;5;28mself\u001b[39m._times_executed += \u001b[32m1\u001b[39m\n\u001b[32m    460\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._times_executed > \u001b[38;5;28mself\u001b[39m.max_retry_limit:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\desktop\\new\\main\\Lib\\site-packages\\crewai\\agent.py:434\u001b[39m, in \u001b[36mAgent.execute_task\u001b[39m\u001b[34m(self, task, context, tools)\u001b[39m\n\u001b[32m    430\u001b[39m         result = \u001b[38;5;28mself\u001b[39m._execute_with_timeout(\n\u001b[32m    431\u001b[39m             task_prompt, task, \u001b[38;5;28mself\u001b[39m.max_execution_time\n\u001b[32m    432\u001b[39m         )\n\u001b[32m    433\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m434\u001b[39m         result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_execute_without_timeout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask_prompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    436\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    437\u001b[39m     \u001b[38;5;66;03m# Propagate TimeoutError without retry\u001b[39;00m\n\u001b[32m    438\u001b[39m     crewai_event_bus.emit(\n\u001b[32m    439\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    440\u001b[39m         event=AgentExecutionErrorEvent(\n\u001b[32m   (...)\u001b[39m\u001b[32m    444\u001b[39m         ),\n\u001b[32m    445\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\desktop\\new\\main\\Lib\\site-packages\\crewai\\agent.py:530\u001b[39m, in \u001b[36mAgent._execute_without_timeout\u001b[39m\u001b[34m(self, task_prompt, task)\u001b[39m\n\u001b[32m    520\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_execute_without_timeout\u001b[39m(\u001b[38;5;28mself\u001b[39m, task_prompt: \u001b[38;5;28mstr\u001b[39m, task: Task) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m    521\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Execute a task without a timeout.\u001b[39;00m\n\u001b[32m    522\u001b[39m \n\u001b[32m    523\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    528\u001b[39m \u001b[33;03m        The output of the agent.\u001b[39;00m\n\u001b[32m    529\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m530\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43magent_executor\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    531\u001b[39m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    532\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minput\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    533\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_names\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43magent_executor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtools_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    534\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43magent_executor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtools_description\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    535\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mask_for_human_input\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhuman_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    536\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m    537\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33moutput\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\desktop\\new\\main\\Lib\\site-packages\\crewai\\agents\\crew_agent_executor.py:114\u001b[39m, in \u001b[36mCrewAgentExecutor.invoke\u001b[39m\u001b[34m(self, inputs)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28mself\u001b[39m.ask_for_human_input = \u001b[38;5;28mbool\u001b[39m(inputs.get(\u001b[33m\"\u001b[39m\u001b[33mask_for_human_input\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m))\n\u001b[32m    113\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m     formatted_answer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_invoke_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m:\n\u001b[32m    116\u001b[39m     \u001b[38;5;28mself\u001b[39m._printer.print(\n\u001b[32m    117\u001b[39m         content=\u001b[33m\"\u001b[39m\u001b[33mAgent failed to reach a final answer. This is likely a bug - please report it.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    118\u001b[39m         color=\u001b[33m\"\u001b[39m\u001b[33mred\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    119\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\desktop\\new\\main\\Lib\\site-packages\\crewai\\agents\\crew_agent_executor.py:208\u001b[39m, in \u001b[36mCrewAgentExecutor._invoke_loop\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    205\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    206\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m e.\u001b[34m__class__\u001b[39m.\u001b[34m__module__\u001b[39m.startswith(\u001b[33m\"\u001b[39m\u001b[33mlitellm\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    207\u001b[39m         \u001b[38;5;66;03m# Do not retry on litellm errors\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m208\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    209\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_context_length_exceeded(e):\n\u001b[32m    210\u001b[39m         handle_context_length(\n\u001b[32m    211\u001b[39m             respect_context_window=\u001b[38;5;28mself\u001b[39m.respect_context_window,\n\u001b[32m    212\u001b[39m             printer=\u001b[38;5;28mself\u001b[39m._printer,\n\u001b[32m   (...)\u001b[39m\u001b[32m    216\u001b[39m             i18n=\u001b[38;5;28mself\u001b[39m._i18n,\n\u001b[32m    217\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\desktop\\new\\main\\Lib\\site-packages\\crewai\\agents\\crew_agent_executor.py:154\u001b[39m, in \u001b[36mCrewAgentExecutor._invoke_loop\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    143\u001b[39m     formatted_answer = handle_max_iterations_exceeded(\n\u001b[32m    144\u001b[39m         formatted_answer,\n\u001b[32m    145\u001b[39m         printer=\u001b[38;5;28mself\u001b[39m._printer,\n\u001b[32m   (...)\u001b[39m\u001b[32m    149\u001b[39m         callbacks=\u001b[38;5;28mself\u001b[39m.callbacks,\n\u001b[32m    150\u001b[39m     )\n\u001b[32m    152\u001b[39m enforce_rpm_limit(\u001b[38;5;28mself\u001b[39m.request_within_rpm_limit)\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m answer = \u001b[43mget_llm_response\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    155\u001b[39m \u001b[43m    \u001b[49m\u001b[43mllm\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    156\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    157\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    158\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprinter\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_printer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfrom_task\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtask\u001b[49m\n\u001b[32m    160\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    161\u001b[39m formatted_answer = process_llm_response(answer, \u001b[38;5;28mself\u001b[39m.use_stop_words)\n\u001b[32m    163\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(formatted_answer, AgentAction):\n\u001b[32m    164\u001b[39m     \u001b[38;5;66;03m# Extract agent fingerprint if available\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\desktop\\new\\main\\Lib\\site-packages\\crewai\\utilities\\agent_utils.py:160\u001b[39m, in \u001b[36mget_llm_response\u001b[39m\u001b[34m(llm, messages, callbacks, printer, from_task, from_agent)\u001b[39m\n\u001b[32m    153\u001b[39m     answer = llm.call(\n\u001b[32m    154\u001b[39m         messages,\n\u001b[32m    155\u001b[39m         callbacks=callbacks,\n\u001b[32m    156\u001b[39m         from_task=from_task,\n\u001b[32m    157\u001b[39m         from_agent=from_agent,\n\u001b[32m    158\u001b[39m     )\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m answer:\n\u001b[32m    162\u001b[39m     printer.print(\n\u001b[32m    163\u001b[39m         content=\u001b[33m\"\u001b[39m\u001b[33mReceived None or empty response from LLM call.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    164\u001b[39m         color=\u001b[33m\"\u001b[39m\u001b[33mred\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    165\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\desktop\\new\\main\\Lib\\site-packages\\crewai\\utilities\\agent_utils.py:153\u001b[39m, in \u001b[36mget_llm_response\u001b[39m\u001b[34m(llm, messages, callbacks, printer, from_task, from_agent)\u001b[39m\n\u001b[32m    151\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Call the LLM and return the response, handling any invalid responses.\"\"\"\u001b[39;00m\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m     answer = \u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    154\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    155\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    156\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfrom_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrom_task\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    157\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfrom_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrom_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    158\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    160\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\desktop\\new\\main\\Lib\\site-packages\\crewai\\llm.py:1019\u001b[39m, in \u001b[36mLLM.call\u001b[39m\u001b[34m(self, messages, tools, callbacks, available_functions, from_task, from_agent)\u001b[39m\n\u001b[32m   1015\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._handle_streaming_response(\n\u001b[32m   1016\u001b[39m             params, callbacks, available_functions, from_task, from_agent\n\u001b[32m   1017\u001b[39m         )\n\u001b[32m   1018\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1019\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_handle_non_streaming_response\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1020\u001b[39m \u001b[43m            \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mavailable_functions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_task\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_agent\u001b[49m\n\u001b[32m   1021\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1023\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m LLMContextLengthExceededException:\n\u001b[32m   1024\u001b[39m     \u001b[38;5;66;03m# Re-raise LLMContextLengthExceededException as it should be handled\u001b[39;00m\n\u001b[32m   1025\u001b[39m     \u001b[38;5;66;03m# by the CrewAgentExecutor._invoke_loop method, which can then decide\u001b[39;00m\n\u001b[32m   1026\u001b[39m     \u001b[38;5;66;03m# whether to summarize the content or abort based on the respect_context_window flag\u001b[39;00m\n\u001b[32m   1027\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\desktop\\new\\main\\Lib\\site-packages\\crewai\\llm.py:813\u001b[39m, in \u001b[36mLLM._handle_non_streaming_response\u001b[39m\u001b[34m(self, params, callbacks, available_functions, from_task, from_agent)\u001b[39m\n\u001b[32m    807\u001b[39m \u001b[38;5;66;03m# --- 1) Make the completion call\u001b[39;00m\n\u001b[32m    808\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    809\u001b[39m     \u001b[38;5;66;03m# Attempt to make the completion call, but catch context window errors\u001b[39;00m\n\u001b[32m    810\u001b[39m     \u001b[38;5;66;03m# and convert them to our own exception type for consistent handling\u001b[39;00m\n\u001b[32m    811\u001b[39m     \u001b[38;5;66;03m# across the codebase. This allows CrewAgentExecutor to handle context\u001b[39;00m\n\u001b[32m    812\u001b[39m     \u001b[38;5;66;03m# length issues appropriately.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m813\u001b[39m     response = \u001b[43mlitellm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletion\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    815\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ContextWindowExceededError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    816\u001b[39m     \u001b[38;5;66;03m# Convert litellm's context window error to our own exception type\u001b[39;00m\n\u001b[32m    817\u001b[39m     \u001b[38;5;66;03m# for consistent handling in the rest of the codebase\u001b[39;00m\n\u001b[32m    818\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m LLMContextLengthExceededException(\u001b[38;5;28mstr\u001b[39m(e))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\desktop\\new\\main\\Lib\\site-packages\\litellm\\utils.py:1330\u001b[39m, in \u001b[36mclient.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1326\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m logging_obj:\n\u001b[32m   1327\u001b[39m     logging_obj.failure_handler(\n\u001b[32m   1328\u001b[39m         e, traceback_exception, start_time, end_time\n\u001b[32m   1329\u001b[39m     )  \u001b[38;5;66;03m# DO NOT MAKE THREADED - router retry fallback relies on this!\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1330\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\desktop\\new\\main\\Lib\\site-packages\\litellm\\utils.py:1205\u001b[39m, in \u001b[36mclient.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1203\u001b[39m         print_verbose(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError while checking max token limit: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m   1204\u001b[39m \u001b[38;5;66;03m# MODEL CALL\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1205\u001b[39m result = \u001b[43moriginal_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1206\u001b[39m end_time = datetime.datetime.now()\n\u001b[32m   1207\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _is_streaming_request(\n\u001b[32m   1208\u001b[39m     kwargs=kwargs,\n\u001b[32m   1209\u001b[39m     call_type=call_type,\n\u001b[32m   1210\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\desktop\\new\\main\\Lib\\site-packages\\litellm\\main.py:3427\u001b[39m, in \u001b[36mcompletion\u001b[39m\u001b[34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, web_search_options, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, thinking, **kwargs)\u001b[39m\n\u001b[32m   3424\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[32m   3425\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   3426\u001b[39m     \u001b[38;5;66;03m## Map to OpenAI Exception\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3427\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exception_type(\n\u001b[32m   3428\u001b[39m         model=model,\n\u001b[32m   3429\u001b[39m         custom_llm_provider=custom_llm_provider,\n\u001b[32m   3430\u001b[39m         original_exception=e,\n\u001b[32m   3431\u001b[39m         completion_kwargs=args,\n\u001b[32m   3432\u001b[39m         extra_kwargs=kwargs,\n\u001b[32m   3433\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\desktop\\new\\main\\Lib\\site-packages\\litellm\\main.py:1097\u001b[39m, in \u001b[36mcompletion\u001b[39m\u001b[34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, web_search_options, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, thinking, **kwargs)\u001b[39m\n\u001b[32m   1095\u001b[39m     model = deployment_id\n\u001b[32m   1096\u001b[39m     custom_llm_provider = \u001b[33m\"\u001b[39m\u001b[33mazure\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1097\u001b[39m model, custom_llm_provider, dynamic_api_key, api_base = \u001b[43mget_llm_provider\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1098\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1099\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1100\u001b[39m \u001b[43m    \u001b[49m\u001b[43mapi_base\u001b[49m\u001b[43m=\u001b[49m\u001b[43mapi_base\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1101\u001b[39m \u001b[43m    \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m=\u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1102\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1104\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   1105\u001b[39m     provider_specific_header \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1106\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m provider_specific_header[\u001b[33m\"\u001b[39m\u001b[33mcustom_llm_provider\u001b[39m\u001b[33m\"\u001b[39m] == custom_llm_provider\n\u001b[32m   1107\u001b[39m ):\n\u001b[32m   1108\u001b[39m     headers.update(provider_specific_header[\u001b[33m\"\u001b[39m\u001b[33mextra_headers\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\desktop\\new\\main\\Lib\\site-packages\\litellm\\litellm_core_utils\\get_llm_provider_logic.py:391\u001b[39m, in \u001b[36mget_llm_provider\u001b[39m\u001b[34m(model, custom_llm_provider, api_base, api_key, litellm_params)\u001b[39m\n\u001b[32m    389\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    390\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, litellm.exceptions.BadRequestError):\n\u001b[32m--> \u001b[39m\u001b[32m391\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    392\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    393\u001b[39m         error_str = (\n\u001b[32m    394\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mGetLLMProvider Exception - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33moriginal model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    395\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\desktop\\new\\main\\Lib\\site-packages\\litellm\\litellm_core_utils\\get_llm_provider_logic.py:368\u001b[39m, in \u001b[36mget_llm_provider\u001b[39m\u001b[34m(model, custom_llm_provider, api_base, api_key, litellm_params)\u001b[39m\n\u001b[32m    366\u001b[39m     error_str = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m Pass model as E.g. For \u001b[39m\u001b[33m'\u001b[39m\u001b[33mHuggingface\u001b[39m\u001b[33m'\u001b[39m\u001b[33m inference endpoints pass in `completion(model=\u001b[39m\u001b[33m'\u001b[39m\u001b[33mhuggingface/starcoder\u001b[39m\u001b[33m'\u001b[39m\u001b[33m,..)` Learn more: https://docs.litellm.ai/docs/providers\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    367\u001b[39m     \u001b[38;5;66;03m# maps to openai.NotFoundError, this is raised when openai does not recognize the llm\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m368\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m litellm.exceptions.BadRequestError(  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m    369\u001b[39m         message=error_str,\n\u001b[32m    370\u001b[39m         model=model,\n\u001b[32m    371\u001b[39m         response=httpx.Response(\n\u001b[32m    372\u001b[39m             status_code=\u001b[32m400\u001b[39m,\n\u001b[32m    373\u001b[39m             content=error_str,\n\u001b[32m    374\u001b[39m             request=httpx.Request(method=\u001b[33m\"\u001b[39m\u001b[33mcompletion\u001b[39m\u001b[33m\"\u001b[39m, url=\u001b[33m\"\u001b[39m\u001b[33mhttps://github.com/BerriAI/litellm\u001b[39m\u001b[33m\"\u001b[39m),  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m    375\u001b[39m         ),\n\u001b[32m    376\u001b[39m         llm_provider=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    377\u001b[39m     )\n\u001b[32m    378\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m api_base \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(api_base, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    379\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\n\u001b[32m    380\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mapi base needs to be a string. api_base=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(api_base)\n\u001b[32m    381\u001b[39m     )\n",
      "\u001b[31mBadRequestError\u001b[39m: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=models/gemini-1.5-flash-latest\n Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers"
     ]
    }
   ],
   "source": [
    "result = crew.kickoff(inputs={\"query\": \"Summarize the ingested document in detail\"})\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
